[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "lamnguyenx",
    "section": "",
    "text": "about me\nMy name is Tung Lam Nguyen. I‚Äôm an AI backend engineer with 6.5 years of experience building voice processing systems that are performant, scalable, and easy to maintain.",
    "crumbs": [
      "about me"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html",
    "href": "advanced-context-engineering.html",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "",
    "text": "1.1 Grounding Context from AI Engineer\nIt seems pretty well-accepted that AI coding tools struggle with real production codebases. The Stanford study on AI‚Äôs impact on developer productivity found:\nThe common response is somewhere between the pessimist ‚Äúthis will never work‚Äù and the more measured ‚Äúmaybe someday when there are smarter models.‚Äù\nAfter several months of tinkering, I‚Äôve found that you can get really far with today‚Äôs models if you embrace core context engineering principles.\nThis isn‚Äôt another ‚Äú10x your productivity‚Äù pitch. I tend to be pretty measured when it comes to interfacing with the ai hype machine. But we‚Äôve stumbled into workflows that leave me with considerable optimism for what‚Äôs possible. We‚Äôve gotten claude code to handle 300k LOC Rust codebases, ship a week‚Äôs worth of work in a day, and maintain code quality that passes expert review. We use a family of techniques I call ‚Äúfrequent intentional compaction‚Äù - deliberately structuring how you feed context to the AI throughout the development process.\nI am now fully convinced that AI for coding is not just for toys and prototypes, but rather a deeply technical engineering craft.\nVideo Version: If you prefer video, this post is based on a talk given at Y Combinator on August 20th\nTwo talks from AI Engineer 2025 fundamentally shaped my thinking about this problem.\nThe first is Sean Grove‚Äôs talk on ‚ÄúSpecs are the new code‚Äù and the second is the Stanford study on AI‚Äôs impact on developer productivity.\nSean argued that we‚Äôre all vibe coding wrong. The idea of chatting with an AI agent for two hours, specifying what you want, and then throwing away all the prompts while committing only the final code‚Ä¶ is like a Java developer compiling a JAR and checking in the compiled binary while throwing away the source.\nSean proposes that in the AI future, the specs will become the real code. That in two years, you‚Äôll be opening python files in your IDE with about the same frequency that, today, you might open up a hex editor to read assembly (which, for most of us, is never).\nYegor‚Äôs talk on developer productivity tackled an orthogonal problem. They analyzed commits from 100k developers and found, among other things,\nThis matched what I heard talking with founders:\nThe general vibe on AI-coding for hard stuff tends to be\nHeck even Amjad was on a lenny‚Äôs podcast 9 months ago talking about how PMs use Replit agent to prototype new stuff and then they hand it off to engineers to implement for production. (Disclaimer: i haven‚Äôt caught up with him recently (ok, ever), this stance may have changed)\nWhenever I hear ‚ÄúMaybe someday when the models are smart‚Äù I generally leap to exclaim that‚Äôs what context engineering is all about: getting the most out of today‚Äôs models.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html#advanced-context-engineering-for-coding-agents",
    "href": "advanced-context-engineering.html#advanced-context-engineering-for-coding-agents",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "1.4 Advanced Context Engineering for Coding Agents",
    "text": "1.4 Advanced Context Engineering for Coding Agents\nWhat we needed was:\n\nAI that Works Well in Brownfield Codebases\nAI that Solves Complex Problems\nNo Slop\nMaintain Mental Alignment across the team\n\n(And yeah sure, let‚Äôs try to spend as many tokens as possible.)\nI‚Äôll dive into:\n\nwhat we learned applying context engineering to coding agents\nthe dimensions along which using these agents is a deeply technical craft\nwhy I don‚Äôt believe these approaches are generalizable\nthe number of times I‚Äôve been repeatedly proven wrong about (3)\n\n\n1.4.1 But first: The Naive Way to manage agent context\nMost of us start by using a coding agent like a chatbot. You talk (or drunkenly shout) back and forth with it, vibing your way through a problem until you either run out of context, give up, or the agent starts apologizing.\n\n\n\nNaive context management approach\n\n\nA slightly smarter way is to just start over when you get off track, discarding your session and starting a new one, perhaps with a little more steering in the prompt.\n\n[original prompt], but make sure you use XYZ approach, because ABC approach won‚Äôt work\n\n\n\n\nRestarting sessions when off-track\n\n\n\n\n1.4.2 Slightly Smarter: Intentional Compaction\nYou have probably done something I‚Äôve come to call ‚Äúintentional compaction‚Äù. Whether you‚Äôre on track or not, as your context starts to fill up, you probably want to pause your work and start over with a fresh context window. To do this, you might use a prompt like\n\n‚ÄúWrite everything we did so far to progress.md, ensure to note the end goal, the approach we‚Äôre taking, the steps we‚Äôve done so far, and the current failure we‚Äôre working on‚Äù\n\n\n\n\nIntentional compaction workflow\n\n\nYou can also use commit messages for intentional compaction.\n\n\n1.4.3 What Exactly Are We Compacting?\nWhat eats up context?\n\nSearching for files\nUnderstanding code flow\nApplying edits\nTest/build logs\nHuge JSON blobs from tools\n\nAll of these can flood the context window. Compaction is simply distilling them into structured artifacts.\nA good output for an intentional compaction might include something like\n\n\n\nExample of good compaction output\n\n\n\n\n1.4.4 Why obsess over context?\nAs we went deep on in 12-factor agents, LLMs are stateless functions. The only thing that affects the quality of your output (without training/tuning models themselves) is the quality of the inputs.\nThis is just as true for wielding coding agents as it is for general agent design, you just have a smaller problem space, and rather than building agents, we‚Äôre talking about using agents.\nAt any given point, a turn in an agent like claude code is a stateless function call. Context window in, next step out.\n\n\n\nImage\n\n\nThat is, the contents of your context window are the ONLY lever you have to affect the quality of your output. So yeah, it‚Äôs worth obsessing over.\nYou should optimize your context window for:\n\nCorrectness\nCompleteness\nSize\nTrajectory\n\nPut another way, the worst things that can happen to your context window, in order, are:\n\nIncorrect Information\nMissing Information\nToo much Noise\n\nIf you like equations, here‚Äôs a dumb one you can reference:\n\nEquation diagram not available\n\nAs Geoff Huntley puts it,\n\nThe name of the game is that you only have approximately 170k of context window to work with. So it‚Äôs essential to use as little of it as possible. The more you use the context window, the worse the outcomes you‚Äôll get.\n\nGeoff‚Äôs solution to this engineering constraint is a technique he calls Ralph Wiggum as a Software Engineer, which basically involves running an agent in a while loop forever with a simple prompt.\nwhile :; do\n  cat PROMPT.md | npx --yes @sourcegraph/amp\ndone\nIf you wanna learn more about ralph or what‚Äôs in PROMPT.md, you can check out Geoff‚Äôs post or dive into the project that @simonfarshid, @lantos1618, @AVGVSTVS96 and I built at last weekend‚Äôs YC Agents Hackathon, which was able to (mostly) port BrowserUse to TypeScript overnight\nGeoff describes ralph as a ‚Äúhilariously dumb‚Äù solution to the context window problem. I‚Äôm not entirely sure that it is dumb.\n\n\n1.4.5 Back to compaction: Using Sub-Agents\nSubagents are another way to manage context, and generic subagents (i.e.¬†not custom ones) have been a feature of claude code and many coding CLIs since the early days.\nSubagents are not about playing house and anthropomorphizing roles. Subagents are about context control.\nThe most common/straightforward use case for subagents is to let you use a fresh context window to do finding/searching/summarizing that enables the parent agent to get straight to work without clouding its context window with Glob / Grep / Read / etc calls.\n\n\n(video not playing on mobile? expand for the static image version)\n\n\nStatic image version not available\n\n\nThe ideal subagent response probably looks similar to the ideal ad-hoc compaction from above\n\n\n\nImage\n\n\nGetting a subagent to return this is not trivial:\n\n\n\nImage\n\n\n\n\n1.4.6 What works even better: Frequent Intentional Compaction\nThe techniques I want to talk about and that we‚Äôve adopted in the last few months fall under what I call ‚Äúfrequent intentional compaction‚Äù.\nEssentially, this means designing your ENTIRE WORKFLOW around context management, and keeping utilization in the 40%-60% range (depends on complexity of the problem ).\nThe way we do it is to split into three (ish) steps.\nI say ‚Äúish‚Äù because sometimes we skip the research and go straight to planning, and sometimes we‚Äôll do multiple passes of compacted research before we‚Äôre ready to implement.\nI‚Äôll share example outputs of each step in a concrete example below. For a given feature or bug, we‚Äôll tend to do:\nResearch\nUnderstand the codebase, the files relevant to the issue, and how information flows, and perhaps potential causes of a problem.\nhere‚Äôs our research prompt. It currently uses custom subagents, but in other repos I use a more generic version that uses the claude code Task() tool with general-agent. The generic one works almost as well.\nPlan\nOutline the exact steps we‚Äôll take to fix the issue, and the files we‚Äôll need to edit and how, being super precise about the testing / verification steps in each phase.\nThis is the prompt we use for planning.\nImplement\nStep through the plan, phase by phase. For complex work, I‚Äôll often compact the current status back into the original plan file after each implementation phase is verified.\nThis is the implementation prompt we use.\nAside - if you‚Äôve been hearing a lot about git worktrees, this is the only step that needs to be done in a worktree. We tend to do everything else on main.\nHow we manage/share the markdown files\nI will skip this part for brevity but feel free to launch a claude session in humanlayer/humanlayer and ask how the ‚Äúthoughts tool‚Äù works.\n\n\n1.4.7 Putting this into practice\nI do a weekly live-coding session with @vaibhav where we whiteboard and code up a solution to an advanced AI Engineering problem. It‚Äôs one of the highlights of my week.\nSeveral weeks ago, I decided to share some more about the process, curious if our in-house techniques could one-shot a fix to a 300k LOC Rust codebase for BAML, a programming language for working with LLMs. I picked out an (admittedly small-ish) bug from the @BoundaryML repo and got to work.\nYou can watch the episode to learn more about the process, but to outline it:\nWorth noting: I am at best an amateur Rust dev, and I have never worked in the BAML codebase before.\n\n1.4.7.1 The research\n\nI created a piece of research, I read it. Claude decided the bug was invalid and the codebase was correct.\nI threw that research out and kicked off a new one, with more steering.\nhere is the final research doc i ended up using\n\n\n\n1.4.7.2 The plans\n\nWhile the research was running, I got impatient and kicked off a plan, with no research, to see if claude could go straight to an implementation plan - you can see it here\nWhen the research was done, I kicked off another implementation plan that used the research results - you can see it here\n\nThe plans are both fairly short, but they differ significantly. They fix the issue in different ways, and have different testing approaches. Without going too much into detail, they both ‚Äúwould have worked‚Äù but the one built with research fixed the problem in the best place and prescribed testing that was in line with the codebase conventions.\n\n\n1.4.7.3 The implementation\n\nThis was all happening the night before the podcast recording. I ran both plans in parallel and submitted both as PRs before signing off for the night.\n\nBy the time we were on the show at 10am PT the next day, the PR from the plan with the research was already approved by @aaron, who didn‚Äôt even know I was doing a bit for a podcast üôÇ. We closed the other one.\nSo out of our original 4 goals, we hit:\n\n‚úÖ Works in brownfield codebases (300k LOC rust project)\nSolves complex problems\n‚úÖ no slop (pr merged)\nKeeps mental alignment\n\n\n\n\n1.4.8 Solving complex problems\nVaibhav was still skeptical, and I wanted to see if we could solve a more complex problem.\nSo a few weeks later, the two of us spent 7 hours (3 hours on research/plans, 4 hours on implementation) and shipped 35k LOC to add cancellation and wasm support to BAML. The cancelation PR just got merged last week. The WASM one is still open, but has a working demo of calling the wasm-compiled rust runtime from a JS app in the browser.\nWhile the cancelation PR required a little more love to take things over the line, we got incredible progress in just a day. Vaibhav estimated that each of these PRs would have been 3-5 days of work for a senior engineer on the BAML team to complete.\n‚úÖ So we can solve complex problems too.\n\n\n1.4.9 This is not Magic\nRemember that part in the example where I read the research and threw it out cause it was wrong? Or me and Vaibhav sitting DEEPLY ENGAGED FOR 7 HOURS? You have to engage with your task when you‚Äôre doing this or it WILL NOT WORK.\nThere‚Äôs a certain type of person who is always looking for the one magic prompt that will solve all their problems. It doesn‚Äôt exist.\nFrequent Intentional Compaction via a research/plan/implement flow will make your performance better, but what makes it good enough for hard problems is that you build high-leverage human review into your pipeline.\n\nHuman leverage diagram not available\n\n\n\n1.4.10 Eggs on Faces\nA few weeks back, @blakesmith and I sat down for 7 hours and tried to remove hadoop dependencies from parquet java - the deep dive on everything that went wrong and my theories as to why, I‚Äôll save for another post, suffice it to say that it did not go well. The tl;dr is that the research steps didn‚Äôt go deep enough through the dependency tree, and assumed classes could be moved upstream without introducing deeply nested hadoop dependencies.\nThere are big hard problems you cannot just prompt your way through in 7 hours, and we‚Äôre still curiously and excitedly hacking on pushing the boundaries with friends and partners. I think the other learning here is that you probably need at least one person who is an expert in the codebase, and for this case, that was neither of us.\n\n\n1.4.11 On Human Leverage\nIf there‚Äôs one thing you take away from all this, let it be this:\nA bad line of code is‚Ä¶ a bad line of code. But a bad line of a plan could lead to hundreds of bad lines of code. And a bad line of research, a misunderstanding of how the codebase works or where certain functionality is located, could land you with thousands of bad lines of code.\n\n\n\nImage\n\n\nSo you want to focus human effort and attention on the HIGHEST LEVERAGE parts of the pipeline.\n\nLeverage pipeline diagram not available\n\nWhen you review the research and the plans, you get more leverage than you do when you review the code. (By the way, one of our primary focuses @ humanlayer is helping teams build and leverage high-quality workflow prompts and crafting great collaboration workflows for ai-generated code and specs).\n\n\n1.4.12 What is code review for?\nPeople have a lot of different opinions on what code review is for.\nI prefer Blake Smith‚Äôs framing in Code Review Essentials for Software Teams, where he says the most important part of code review is mental alignment - keeping members of the team on the page as to how the code is changing and why.\n\n\n\nImage\n\n\nRemember those 2k line golang PRs? I cared about them being correct and well designed, but the biggest source of internal unrest and frustration on the team was the lack of mental alignment. I was starting to lose touch with what our product was and how it worked.\nI would expect that anyone who‚Äôs worked with a very productive AI coder has had this experience.\nThis is actually the most important part of research/plan/implement to us. A guaranteed side effect of everyone shipping way more code is that a much larger proportion of your codebase is going to be unfamiliar to any given engineer at any point in time.\nI won‚Äôt even try to convince you that research/plan/implement is the right approach for most teams - it probably isn‚Äôt. But you ABSOLUTELY need an engineering process that\n\nkeeps team members on the same page\nenables team members to quickly learn about unfamiliar parts of the codebase\n\nFor most teams, this is pull requests and internal docs. For us, it‚Äôs now specs, plans, and research.\nI can‚Äôt read 2000 lines of golang daily. But I can read 200 lines of a well-written implementation plan.\nI can‚Äôt go spelunking through 40+ files of daemon code for an hour+ when something is broken (okay, I can, but I don‚Äôt want to). I can steer a research prompt to give me the speed-run on where I should be looking and why.\n\n\n1.4.13 Recap\nBasically we got everything we needed.\n\n‚úÖ Works in brownfield codebases\n‚úÖ Solves complex problems\n‚úÖ No slop\n‚úÖ Maintains mental alignment\n\n(oh, and yeah, our team of three is averaging about $12k on opus per month)\nSo you don‚Äôt think I‚Äôm just another hyped up mustachio‚Äôd sales guy, I‚Äôll note that this does not work perfectly for every problem (we‚Äôll be back for another round sound, parquet-java).\nIn August the whole team spent 2 weeks spinning circles on a really tricky race condition that spiraled into a rabbit hole of issues with MCP sHTTP keepalives in golang and a whole bunch of other dead ends.\nBut that‚Äôs the exception now. In general, this works well for us. Our intern shipped 2 PRs on his first day, and 10 on his 8th day. I was genuinely skeptical that it would work for anyone else, but me and Vaibhav shipped 35k LOC of working BAML code in 7 hours. (And if you haven‚Äôt met Vaibhav, he‚Äôs one of the most meticulous engineers I know when it comes to code design and quality.)\n\n\n1.4.14 What‚Äôs coming\nI‚Äôm reasonably confident that coding agents will be commoditized.\nThe hard part will be the team and workflow transformation. Everything about collaboration will change in a world where AI writes 99% of our code.\nAnd I believe pretty strongly that if you don‚Äôt figure this out, you‚Äôre gonna get lapped by someone who did.\n\n\n1.4.15 okay so clearly you have something to sell me\nWe‚Äôre pretty bullish on spec-first, agentic workflows, so we‚Äôre building tools to make it easier. Among many things, I‚Äôm obsessed with the problem of scaling these ‚Äúfrequent intentional compaction‚Äù workflows collaboratively across large teams.\nToday, we‚Äôre launching CodeLayer, our new ‚Äúpost-IDE IDE‚Äù in private beta - think ‚ÄúSuperhuman for claude code‚Äù. If you‚Äôre a fan of Superhuman and/or vim mode and you‚Äôre ready to move beyond ‚Äúvibe coding‚Äù and get serious about building with agents, we‚Äôd love to have you join the waitlist.\nSign up at https://humanlayer.dev.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html#for-oss-maintainers---lets-ship-something-together",
    "href": "advanced-context-engineering.html#for-oss-maintainers---lets-ship-something-together",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "1.5 For OSS Maintainers - lets ship something together",
    "text": "1.5 For OSS Maintainers - lets ship something together\nIf you are a maintainer on a complex OSS project and based in the bay area, my open offer - I will pair with you in-person in SF for 7 hours on a saturday and see if we can ship something big.\nI get a lot of learning about the limitations and where these techniques fall short (and, with any luck, a working merged PR that adds a ton of value that I can point to). You get to learn the workflow in the only way I‚Äôve found that works well - direct 1x1 pairing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html#for-engineering-leaders",
    "href": "advanced-context-engineering.html#for-engineering-leaders",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "1.6 For Engineering Leaders",
    "text": "1.6 For Engineering Leaders\nIf you or someone you know is an engineering leader that wants to 10x their team‚Äôs productivity with AI, we‚Äôre forward-deploying with ~10-25 person eng orgs to help teams make the culture/process/tech shift needed to transition to an ai-first coding world.\n\n1.6.1 Thanks\n\nThanks to all the friends and founders who‚Äôve listened through early ramble-y versions of this post - Adam, Josh, Andrew, and many many more\nThanks Sundeep for weathering this wacky storm\nThanks Allison, Geoff, and Gerred for dragging us kicking and screaming into the future",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html#grounding-context-from-ai-engineer",
    "href": "advanced-context-engineering.html#grounding-context-from-ai-engineer",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "",
    "text": "That AI tools often lead to a lot of rework, diminishing the perceived productivity gains\n\n\n\n\nAI coding productivity research findings\n\n\n\nThat AI tools work well for greenfield projects, but are often counter-productive for brownfield codebases and complex tasks\n\n\n\n\nAI tools performance across different project types\n\n\n\n\n‚ÄúToo much slop.‚Äù\n‚ÄúTech debt factory.‚Äù\n‚ÄúDoesn‚Äôt work in big repos.‚Äù\n‚ÄúDoesn‚Äôt work for complex systems.‚Äù\n\n\n\nMaybe someday, when models are smarter‚Ä¶",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html#whats-actually-possible-today",
    "href": "advanced-context-engineering.html#whats-actually-possible-today",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "1.2 What‚Äôs actually possible today",
    "text": "1.2 What‚Äôs actually possible today\nI‚Äôll deep dive on this a bit futher down, but to prove this isn‚Äôt just theory, let me outline a concrete example. A few weeks ago, I decided to test our techniques on BAML, a 300k LOC Rust codebase for a programming language that works with LLMs. I‚Äôm at best an amateur Rust dev and had never touched the BAML codebase before.\nWithin an hour or so, I had a PR fixing a bug which was approved by the maintainer the next morning. A few weeks later, @hellovai and I paired on shipping 35k LOC to BAML, adding cancellation support and WASM compilation - features the team estimated would take a senior engineer 3-5 days each. We got both draft prs ready in about 7 hours.\nAgain, this is all built around a workflow we call frequent intentional compaction - essentially designing your entire development process around context management, keeping utilization in the 40-60% range, and building in high-leverage human review at exactly the right points. We use a ‚Äúresearch, plan, implement‚Äù workflow, but the core capabilities/learnings here are FAR more general than any specific workflow or set of prompts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  },
  {
    "objectID": "advanced-context-engineering.html#our-weird-journey-to-get-here",
    "href": "advanced-context-engineering.html#our-weird-journey-to-get-here",
    "title": "1¬† Getting AI to Work in Complex Codebases",
    "section": "1.3 Our weird journey to get here",
    "text": "1.3 Our weird journey to get here\nI was working with one of the most productive AI coders I‚Äôve ever met. Every few days they‚Äôd drop 2000-line Go PRs. And this wasn‚Äôt a nextjs app or a CRUD API. This was complex, race-prone systems code that did JSON RPC over unix sockets and managed streaming stdio from forked unix processes (mostly claude code sdk processes, more on that later üôÇ).\nThe idea of carefully reading 2,000 lines of complex Go code every few days was simply not sustainable. I was starting to feel a bit like Mitchell Hashimoto when he added the AI contributions must be disclosed rules for ghostty.\nOur approach was to adopt something like sean‚Äôs spec-driven development.\nIt was uncomfortable at first. I had to learn to let go of reading every line of PR code. I still read the tests pretty carefully, but the specs became our source of truth for what was being built and why.\nThe transformation took about 8 weeks. It was incredibly uncomfortable for everyone involved, not least of all for me. But now we‚Äôre flying. A few weeks back, I shipped 6 PRs in a day. I can count on one hand the number of times I‚Äôve edited a non-markdown file by hand in the last three months.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting AI to Work in Complex Codebases</span>"
    ]
  }
]